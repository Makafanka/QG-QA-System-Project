# -*- coding: utf-8 -*-
"""QG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CGqZX0zix82jDN5P9eVErLxYeNQp_YSH

Import Dataset
"""

!pip install datasets

!pip install datasets transformers
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

from datasets import load_dataset
squad = load_dataset("squad")

import numpy as np
import pandas as pd

squad
id_cut = squad["train"]["id"][0]
context_cut = squad["train"]["context"][0]
question_cut = squad["train"]["question"][0]

def squad2df(type="train"):

    input_data = squad[type]

    prefix_list = []
    input_text_list = []
    target_text_list = []

    i = 0
    for entry in input_data:
        i += 1
        if i == 20: break
        prefix_list.append("question generation")
        input_text_list.append(entry["context"])
        target_text_list.append(entry["question"])

    output_df = pd.DataFrame(
        {
            "prefix": prefix_list,
            "input_text": input_text_list,
            "target_text": target_text_list,
        }
    )

    return output_df

train = load_pandas_df("train")
print(train)

"""Test print the data set"""

for i, row in enumerate(squad["train"]):
    print("id: ", row["id"])
    print("context: ", row["context"])
    print("question: ", row["question"])
    print('\n')
    if i == 4: break

"""Assign index to every word in the dataset"""

from keras.preprocessing.text import Tokenizer

def vocab_creator(text_lists):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(text_lists)
    dictionary = tokenizer.word_index

    word_to_id = dict()
    id_to_word = dict()
    for word, id in dictionary.items():
        word_to_id[word] = id
        id_to_word[id] = word

    return tokenizer, word_to_id, id_to_word

print("length of context:", len(context_cut))
#print(len(squad["train"]["question"]))
(tokenizer0, word2idx, idx2word) = vocab_creator(context_cut)
print("id_dictionary: ", word2idx)
print("the 500th word in the dictionary is: ",idx2word[100])

"""Convert text in to lists of indices:
  use texts_to_sequences
"""

def bag_of_ids(encoder_text, decoder_text):
  tokenizer = tokenizer0
  encoder_seq = tokenizer.texts_to_sequences(encoder_text)
  decoder_seq = tokenizer.texts_to_sequences(decoder_text)
  return encoder_seq, decoder_seq

encoder_seq, decoder_seq = bag_of_ids(context_cut,question_cut)
print(encoder_seq, decoder_seq)

"""Add padding to list of indices:
fill all lists with zeros.
"""

from keras.preprocessing.sequence import pad_sequences

def add_padding(encoder_sequences, decoder_sequences):

  encoder_seq_padded = pad_sequences(encoder_sequences, dtype='int32', padding='post', truncating='post')
  decoder_seq_padded = pad_sequences(decoder_sequences, dtype='int32', padding='post', truncating='post')

  return encoder_seq_padded, decoder_seq_padded

encoder_seq_padded, decoder_seq_padded = add_padding(encoder_seq, decoder_seq)
print(encoder_seq_padded)

def reformat_data(file="qa_dataset-0.1.csv", doc_path="documents"):
    df = pd.read_csv(file)

    input_text_list = []
    target_text_list = []

    i = 0
    for id in df["document_id"]:
        with open(doc_path + id, 'r') as doc:
            data = doc.read().replace('\n', '')
        input_text_list.append(data)
        i += 1
        if i == 10: break

    output_df = pd.DataFrame(
        {
            "prefix": "question generation",
            "input_text": input_text_list,
            "target_text": df["question"][:10],
        }
    )

    return output_df

# df = pd.read_csv("qa_dataset-0.1.csv")
# i = 0
# for id in df["document_id"]:
#     print(id)
#     i += 1
#     if i == 10: break

# with open('aries_constellation.txt', 'r') as file:
#     data = file.read().replace('\n', '')

# print(data)

print(reformat_data())

max_len = 512
overlap = 128
def prepare_train_features(squad):
#slice context that has length above max_len
    if tokenizer.padding_side == "right":
      tokenized_examples = tokenizer(
        #squad["question"],
        #squad["context"],
        question_cut,
        context_cut,
        truncation="only_second",
        max_length=max_len,
        stride=overlap,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
      )
    else:
      tokenized_examples = tokenizer(
        #squad["context"],
        #squad["question"],
        context_cut,
        question_cut,
        truncation="only_first",
        max_length=max_len,
        stride=overlap,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
      )


    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")

    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    pad_on_right = tokenizer.padding_side == "right"

    for i, offsets in enumerate(offset_mapping):
        input_ids = tokenized_examples["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)

        sequence_ids = tokenized_examples.sequence_ids(i)

        sample_index = sample_mapping[i]
        answers = squad["answers"][sample_index]

        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:

            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])


            token_start_index = 0
            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                token_start_index += 1


            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                token_end_index -= 1


            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples


prepare_train_features(squad["train"])